{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivanathvenigalla/Jaya-Venkatasivanath_INFO5731_Fall2024/blob/main/Venigalla_Jayavenkatasivanath_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76c43f5-b4a3-41c7-9a6e-918a0cf368c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded training data with 3169 rows.\n",
            "\n",
            "First few rows of the raw training data:\n",
            "                                                                                                                                                                  raw\n",
            "1 a          stirring    ,       funny    and          finally transporting re-imagining of             beauty  and   the           beast  and   1930s  horror  films\n",
            "0 apparently reassembled from    the      cutting-room floor   of           any          given          daytime soap  .             NaN    NaN   NaN    NaN       NaN\n",
            "1 jonathan   parker      's      bartleby should       have    been         the          be-all-end-all of      the   modern-office anomie films .      NaN       NaN\n",
            "0 a          fan         film    that     for          the     uninitiated  plays        better         on      video with          the    sound turned down        .\n",
            "1 béart      and         berling are      both         superb  ,            while        huppert        ...     is    magnificent   .      NaN   NaN    NaN       NaN\n",
            "Error processing training data: 'float' object has no attribute 'split'\n",
            "\n",
            "Training Data Sample After Cleaning:\n",
            "                                                                                                                                                                  raw\n",
            "1 a          stirring    ,       funny    and          finally transporting re-imagining of             beauty  and   the           beast  and   1930s  horror  films\n",
            "0 apparently reassembled from    the      cutting-room floor   of           any          given          daytime soap  .             NaN    NaN   NaN    NaN       NaN\n",
            "1 jonathan   parker      's      bartleby should       have    been         the          be-all-end-all of      the   modern-office anomie films .      NaN       NaN\n",
            "0 a          fan         film    that     for          the     uninitiated  plays        better         on      video with          the    sound turned down        .\n",
            "1 béart      and         berling are      both         superb  ,            while        huppert        ...     is    magnificent   .      NaN   NaN    NaN       NaN\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the dataset without any aggressive cleaning for now\n",
        "train_file_path = '/content/stsa-train.txt'\n",
        "\n",
        "try:\n",
        "    # Load the dataset\n",
        "    train_data = pd.read_csv(train_file_path, delimiter=' ', header=None, names=['raw'], on_bad_lines='skip')\n",
        "    print(f\"\\nLoaded training data with {len(train_data)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading training data: {e}\")\n",
        "\n",
        "# Step 2: Inspect the raw data to understand its structure\n",
        "print(\"\\nFirst few rows of the raw training data:\")\n",
        "print(train_data.head())\n",
        "\n",
        "# Step 3: Process the raw data without aggressive filtering\n",
        "try:\n",
        "    # Check if raw data contains any text\n",
        "    if train_data['raw'].notnull().any():\n",
        "        # Instead of strictly extracting the first character as the label, let's explore the possibility of spaces separating the label and text\n",
        "        train_data['label'] = train_data['raw'].apply(lambda x: x.split()[0] if len(x.split()) > 1 else None)  # Take the first word as label\n",
        "        train_data['text'] = train_data['raw'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()) > 1 else None)  # Take the rest as text\n",
        "\n",
        "        # Clean the data (remove rows with missing labels or text)\n",
        "        train_data.dropna(subset=['label', 'text'], inplace=True)\n",
        "\n",
        "        # Ensure labels are numeric and clean any invalid rows\n",
        "        train_data['label'] = pd.to_numeric(train_data['label'], errors='coerce')\n",
        "        train_data.dropna(subset=['label'], inplace=True)  # Drop rows with invalid labels\n",
        "        train_data['label'] = train_data['label'].astype(int)  # Ensure label is integer type\n",
        "\n",
        "        print(f\"Training Data Size After Cleaning: {len(train_data)}\")\n",
        "    else:\n",
        "        print(\"Error: 'raw' column contains null values.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing training data: {e}\")\n",
        "\n",
        "# Debug: Check the structure of train_data after cleaning\n",
        "print(\"\\nTraining Data Sample After Cleaning:\")\n",
        "print(train_data.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf79e98-12d0-49bb-dae7-e71e9ac687f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 lines of the dataset:\n",
            "\n",
            "1 a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "0 apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "0 they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
            "1 this is a visually stunning rumination on love , memory , history and the war between art and commerce .\n",
            "1 jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .\n",
            "1 campanella gets the tone just right -- funny in the middle of sad in the middle of hopeful .\n",
            "0 a fan film that for the uninitiated plays better on video with the sound turned down .\n",
            "1 béart and berling are both superb , while huppert ... is magnificent .\n",
            "0 a little less extreme than in the past , with longer exposition sequences between them , and with fewer gags to break the tedium .\n",
            "0 the film is strictly routine .\n",
            "\n",
            "Loaded dataset with 3169 rows and 2 columns.\n",
            "\n",
            "First few rows of the dataset:\n",
            "                                                                                                                                                           label  \\\n",
            "1 a          stirring    ,       funny    and          finally transporting re-imagining of             beauty  and   the           beast  and   1930s   horror   \n",
            "0 apparently reassembled from    the      cutting-room floor   of           any          given          daytime soap  .             NaN    NaN   NaN        NaN   \n",
            "1 jonathan   parker      's      bartleby should       have    been         the          be-all-end-all of      the   modern-office anomie films .          NaN   \n",
            "0 a          fan         film    that     for          the     uninitiated  plays        better         on      video with          the    sound turned    down   \n",
            "1 béart      and         berling are      both         superb  ,            while        huppert        ...     is    magnificent   .      NaN   NaN        NaN   \n",
            "\n",
            "                                                                                                                                                          text  \n",
            "1 a          stirring    ,       funny    and          finally transporting re-imagining of             beauty  and   the           beast  and   1930s   films  \n",
            "0 apparently reassembled from    the      cutting-room floor   of           any          given          daytime soap  .             NaN    NaN   NaN       NaN  \n",
            "1 jonathan   parker      's      bartleby should       have    been         the          be-all-end-all of      the   modern-office anomie films .         NaN  \n",
            "0 a          fan         film    that     for          the     uninitiated  plays        better         on      video with          the    sound turned      .  \n",
            "1 béart      and         berling are      both         superb  ,            while        huppert        ...     is    magnificent   .      NaN   NaN       NaN  \n",
            "\n",
            "Silhouette Scores:\n",
            "KMeans Silhouette Score: 0.9670084607142421\n",
            "DBSCAN Silhouette Score: Invalid (single label)\n",
            "Hierarchical Clustering Silhouette Score: 0.9670084607142422\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Step 1: Read and inspect the data\n",
        "reviews_path = '/content/stsa-train.txt'\n",
        "\n",
        "# Read the first 10 lines of the dataset to understand its structure\n",
        "with open(reviews_path, 'r') as file:\n",
        "    lines = [file.readline().strip() for _ in range(10)]\n",
        "    print(\"First 10 lines of the dataset:\\n\")\n",
        "    print(\"\\n\".join(lines))\n",
        "\n",
        "# Read the dataset while handling multiple spaces between columns\n",
        "reviews_data = pd.read_csv(reviews_path, sep=r'\\s+', header=None, names=['label', 'text'], on_bad_lines='skip')\n",
        "\n",
        "# Check if data is loaded correctly\n",
        "print(f\"\\nLoaded dataset with {reviews_data.shape[0]} rows and {reviews_data.shape[1]} columns.\")\n",
        "print(\"\\nFirst few rows of the dataset:\\n\", reviews_data.head())\n",
        "\n",
        "# Step 2: Drop rows with NaN values in the 'text' column\n",
        "reviews_data.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "# Step 3: Convert text to feature vectors using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "text_vectors = vectorizer.fit_transform(reviews_data['text'])\n",
        "\n",
        "# Step 4: Apply clustering algorithms\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42).fit(text_vectors)\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5).fit(text_vectors)\n",
        "\n",
        "# Hierarchical clustering\n",
        "linkage_matrix = linkage(text_vectors.toarray(), method='ward')\n",
        "hierarchical_clusters = fcluster(linkage_matrix, t=5, criterion='maxclust')\n",
        "\n",
        "# Step 5: Evaluate clustering using Silhouette Score\n",
        "kmeans_silhouette = silhouette_score(text_vectors, kmeans.labels_)\n",
        "\n",
        "# Only calculate silhouette score for DBSCAN if it has more than one label\n",
        "if len(set(dbscan.labels_)) > 1:\n",
        "    dbscan_silhouette = silhouette_score(text_vectors, dbscan.labels_)\n",
        "else:\n",
        "    dbscan_silhouette = 'Invalid (single label)'\n",
        "\n",
        "hierarchical_silhouette = silhouette_score(text_vectors, hierarchical_clusters)\n",
        "\n",
        "# Print Silhouette Scores\n",
        "print(\"\\nSilhouette Scores:\")\n",
        "print(f\"KMeans Silhouette Score: {kmeans_silhouette}\")\n",
        "print(f\"DBSCAN Silhouette Score: {dbscan_silhouette}\")\n",
        "print(f\"Hierarchical Clustering Silhouette Score: {hierarchical_silhouette}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "The results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT differ based on their methods and data. K-means works well with well-separated, spherical clusters but struggles with irregular shapes or noise. DBSCAN can handle clusters of any shape and noise points but requires proper parameter tuning. Hierarchical clustering offers detailed insights through a dendrogram but becomes inefficient with large datasets. Word2Vec creates dense word vectors based on local context but struggles with complex sentences or out-of-vocabulary words. BERT, a transformer model, captures sentence context well due to its bidirectional nature and performs better in text tasks, though it is more computationally expensive than the others. While K-means and DBSCAN focus on clustering, Word2Vec and BERT are used for embeddings, with BERT excelling in contextual understanding and sentence-level analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This exercise was a valuable experience in applying machine learning techniques for text analysis, particularly sentiment classification and clustering. It allowed me to explore different models, from traditional algorithms like Naive Bayes and SVM to advanced methods like BERT. I found the cross-validation process insightful in understanding model performance and variability. Additionally, the clustering task provided a practical comparison of different methods and their strengths, such as K-means for efficiency versus DBSCAN's handling of noise. Overall, this exercise deepened my understanding of text data processing, model evaluation, and the importance of selecting the right approach for different tasks."
      ],
      "metadata": {
        "id": "FIXVpGiGsS8t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LT_pyFLnsT3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}