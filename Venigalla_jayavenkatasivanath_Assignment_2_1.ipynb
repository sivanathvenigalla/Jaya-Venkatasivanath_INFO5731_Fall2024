{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivanathvenigalla/Jaya-Venkatasivanath_INFO5731_Fall2024/blob/main/Venigalla_jayavenkatasivanath_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p2EY5z-1U76m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b03fb5f-b175-416f-e87e-47a025f16640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved papers starting from offset 0.\n",
            "Retrieved papers starting from offset 100.\n",
            "Retrieved papers starting from offset 200.\n",
            "Retrieved papers starting from offset 300.\n",
            "Retrieved papers starting from offset 400.\n",
            "Retrieved papers starting from offset 500.\n",
            "Retrieved papers starting from offset 600.\n",
            "Retrieved papers starting from offset 700.\n",
            "Retrieved papers starting from offset 800.\n",
            "Retrieved papers starting from offset 900.\n",
            "Request failed with status code 400 at offset 1000.\n",
            "Successfully saved 1000 paper abstracts to information_extraction_papers.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "api_key = \"x2X70VOLvD9dgtxOzZ3Mc50QWwpNPsgu8Kw2pIwR\"\n",
        "\n",
        "request_headers = {\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"x-api-key\": api_key\n",
        "}\n",
        "\n",
        "search_term = \"information extraction\"\n",
        "\n",
        "endpoint_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "request_params = {\n",
        "    \"query\": search_term,\n",
        "    \"fields\": \"title,abstract\",\n",
        "    \"limit\": 100\n",
        "}\n",
        "\n",
        "collected_papers = []\n",
        "\n",
        "for start in range(0, 10000, 100):\n",
        "    request_params[\"offset\"] = start\n",
        "    response = requests.get(endpoint_url, headers=request_headers, params=request_params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json().get(\"data\", [])\n",
        "        if results:\n",
        "            collected_papers.extend(results)\n",
        "            print(f\"Retrieved papers starting from offset {start}.\")\n",
        "        else:\n",
        "            print(f\"No papers found at offset {start}.\")\n",
        "            break\n",
        "    elif response.status_code == 429:\n",
        "        print(f\"Rate limit reached at offset {start}. Pausing briefly.\")\n",
        "        time.sleep(1)\n",
        "        retry_response = requests.get(endpoint_url, headers=request_headers, params=request_params)\n",
        "\n",
        "        if retry_response.status_code == 200:\n",
        "            retry_results = retry_response.json().get(\"data\", [])\n",
        "            if retry_results:\n",
        "                collected_papers.extend(retry_results)\n",
        "                print(f\"Successful retry for offset {start}.\")\n",
        "            else:\n",
        "                print(f\"No data found after retry at offset {start}.\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Retry failed with status code {retry_response.status_code} at offset {start}.\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Request failed with status code {response.status_code} at offset {start}.\")\n",
        "        break\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "paper_data = [(item[\"title\"], item.get(\"abstract\", \"No abstract available\")) for item in collected_papers]\n",
        "\n",
        "csv_file = \"information_extraction_papers.csv\"\n",
        "df = pd.DataFrame(paper_data, columns=[\"Title\", \"Abstract\"])\n",
        "df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Successfully saved {len(paper_data)} paper abstracts to {csv_file}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec787e1-8ab2-4b59-8487-a5eaab26c58e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data has been saved to 'cleaned_information_extraction_papers.csv'\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('information_extraction_papers.csv')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "stopword_set = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token.lower() not in stopword_set]\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmed = [ps.stem(token) for token in tokens]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatized = [wnl.lemmatize(token) for token in stemmed]\n",
        "\n",
        "    # Join the processed words back into a single string\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "df['Abstract'] = df['Abstract'].astype(str).fillna('')\n",
        "df['Processed_Abstract'] = df['Abstract'].apply(preprocess_text)\n",
        "output_file = 'cleaned_information_extraction_papers.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Cleaned data has been saved to '{output_file}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc8e68a-1798-4b2e-d4a3-6ada5f2569e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is saved in analyzed_information_extraction_papers.csv\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK resources (uncomment if you haven't already)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "file = \"cleaned_information_extraction_papers.csv\"\n",
        "data = pd.read_csv(file)\n",
        "\n",
        "# Load the spaCy model for NER and dependency parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging and count POS types\n",
        "def pos_analysis(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)  # POS tagging\n",
        "\n",
        "    # Counting specific POS tags\n",
        "    counts = Counter(tag for word, tag in tagged)\n",
        "    noun_count = sum(counts[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS'])\n",
        "    verb_count = sum(counts[tag] for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
        "    adjective_count = sum(counts[tag] for tag in ['JJ', 'JJR', 'JJS'])\n",
        "    adverb_count = sum(counts[tag] for tag in ['RB', 'RBR', 'RBS'])\n",
        "\n",
        "    pos_counts = {\n",
        "        'Nouns': noun_count,\n",
        "        'Verbs': verb_count,\n",
        "        'Adjectives': adjective_count,\n",
        "        'Adverbs': adverb_count\n",
        "    }\n",
        "\n",
        "    return tagged, pos_counts\n",
        "\n",
        "# Function to perform dependency parsing using spaCy\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "# Function to perform constituency parsing using nltk\n",
        "def constituency_parsing(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "\n",
        "    def tree_to_string(tree):\n",
        "        \"\"\" Convert an NLTK tree to a string to visualize the tree structure. \"\"\"\n",
        "        if isinstance(tree, Tree):\n",
        "            return f\"({tree.label()} {' '.join(tree_to_string(child) for child in tree)})\"\n",
        "        else:\n",
        "            return tree[0]\n",
        "\n",
        "    return tree_to_string(chunked)\n",
        "\n",
        "# Function to extract named entities and their counts\n",
        "def ner_analysis(text):\n",
        "    doc = nlp(text)\n",
        "    entities = Counter(ent.label_ for ent in doc.ents)\n",
        "    return entities, [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Perform analysis for each abstract and store results\n",
        "results = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    abstract = row['Processed_Abstract']\n",
        "\n",
        "    # Skip empty abstracts\n",
        "    if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    # POS Analysis\n",
        "    tagged, pos_counts = pos_analysis(abstract)\n",
        "\n",
        "    # Dependency Parsing\n",
        "    dependency_parse = dependency_parsing(abstract)\n",
        "\n",
        "    # Constituency Parsing\n",
        "    constituency_parse = constituency_parsing(abstract)\n",
        "\n",
        "    # NER Analysis\n",
        "    ner_counts, ner_entities = ner_analysis(abstract)\n",
        "\n",
        "    results.append({\n",
        "        'Abstract': abstract,\n",
        "        'POS Tagged': tagged,\n",
        "        'POS Counts': pos_counts,\n",
        "        'Dependency Parsing': dependency_parse,\n",
        "        'Constituency Parsing': constituency_parse,\n",
        "        'Named Entities': ner_entities,\n",
        "        'NER Counts': ner_counts\n",
        "    })\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"analyzed_information_extraction_papers.csv\", index=False)\n",
        "print(\"Data is saved in analyzed_information_extraction_papers.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#My file will be avaialble in the path once the whole file is run:  /content/analyzed_information_extraction_papers.csv\n",
        "import pandas\n",
        "file_path = \"/content/analyzed_information_extraction_papers.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e3a8f0-cbfb-466a-807d-96d607fb6a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Abstract  \\\n",
            "0  zeroshot inform extract ie aim build ie system...   \n",
            "1  capabl larg languag model llm like chatgpt com...   \n",
            "2  inform extract suffer vari target heterogen st...   \n",
            "3  larg languag model unlock strong multitask cap...   \n",
            "4  humanlik larg languag model llm especi power p...   \n",
            "\n",
            "                                          POS Tagged  \\\n",
            "0  [('zeroshot', 'JJ'), ('inform', 'NN'), ('extra...   \n",
            "1  [('capabl', 'NN'), ('larg', 'NN'), ('languag',...   \n",
            "2  [('inform', 'NN'), ('extract', 'NN'), ('suffer...   \n",
            "3  [('larg', 'NN'), ('languag', 'NN'), ('model', ...   \n",
            "4  [('humanlik', 'NN'), ('larg', 'NN'), ('languag...   \n",
            "\n",
            "                                          POS Counts  \\\n",
            "0  {'Nouns': 66, 'Verbs': 14, 'Adjectives': 25, '...   \n",
            "1  {'Nouns': 65, 'Verbs': 9, 'Adjectives': 25, 'A...   \n",
            "2  {'Nouns': 63, 'Verbs': 4, 'Adjectives': 23, 'A...   \n",
            "3  {'Nouns': 60, 'Verbs': 7, 'Adjectives': 15, 'A...   \n",
            "4  {'Nouns': 67, 'Verbs': 7, 'Adjectives': 27, 'A...   \n",
            "\n",
            "                                  Dependency Parsing  \\\n",
            "0  [('zeroshot', 'compound', 'inform'), ('inform'...   \n",
            "1  [('capabl', 'amod', 'model'), ('larg', 'compou...   \n",
            "2  [('inform', 'compound', 'extract'), ('extract'...   \n",
            "3  [('larg', 'compound', 'model'), ('languag', 'c...   \n",
            "4  [('humanlik', 'compound', 'larg'), ('larg', 'c...   \n",
            "\n",
            "                                Constituency Parsing  \\\n",
            "0  (S zeroshot inform extract ie aim build ie sys...   \n",
            "1  (S capabl larg languag model llm like chatgpt ...   \n",
            "2  (S inform extract suffer vari target heterogen...   \n",
            "3  (S larg languag model unlock strong multitask ...   \n",
            "4  (S humanlik larg languag model llm especi powe...   \n",
            "\n",
            "                                      Named Entities  \\\n",
            "0  [('zeroshot inform', 'ORG'), ('challeng', 'ORG...   \n",
            "1  [('measur', 'GPE'), ('provid highqual', 'PERSO...   \n",
            "2  [('heterogen', 'ORG'), ('abil differ knowledg ...   \n",
            "3  [('recent studi', 'GPE'), ('significantli', 'O...   \n",
            "4  [('humanlik larg languag', 'PERSON'), ('gpt', ...   \n",
            "\n",
            "                                          NER Counts  \n",
            "0    Counter({'ORG': 4, 'CARDINAL': 3, 'PERSON': 1})  \n",
            "1         Counter({'GPE': 1, 'PERSON': 1, 'ORG': 1})  \n",
            "2                   Counter({'PERSON': 2, 'ORG': 1})  \n",
            "3         Counter({'ORG': 3, 'GPE': 1, 'PERSON': 1})  \n",
            "4  Counter({'ORG': 6, 'PERSON': 1, 'CARDINAL': 1,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with enormous datasets for this project was the most difficult part, especially managing the rate constraints while collecting data from APIs. To guarantee that all of the dataset was acquired error-free, attention had to be taken while managing rate restrictions, retries, and offsets during the Semantic Scholar data collection process.\n",
        "\n",
        "The application of several Natural Language Processing (NLP) techniques, such as tokenization, noise removal, and stemming, made the text cleaning phase fun. It provided a greater knowledge of how unprocessed text may be turned into relevant data for analysis. It was also intriguing to undertake syntactic analysis with POS tagging and dependency parsing since it demonstrated how sentence structure can be programmatically evaluated.\n",
        "\n",
        "\n",
        "Although the allotted time for the task seems plenty, there might occasionally be difficulties in guaranteeing efficient data extraction from an API. Considering this, a little extra time would have been beneficial to fully evaluate the data cleaning and analysis procedure.\n"
      ],
      "metadata": {
        "id": "7Nzet1dNvvl-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}