{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivanathvenigalla/Jaya-Venkatasivanath_INFO5731_Fall2024/blob/main/Venigalla_Exercise_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "outputId": "6feb86cd-2a54-4cb7-cd10-10aae1388b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.037*\"india\" + 0.034*\"siva\" + 0.031*\"strength\" + 0.029*\"guntur\" + 0.022*\"globally\"\n",
            "Topic 1: 0.044*\"siva\" + 0.041*\"india\" + 0.030*\"guntur\" + 0.028*\"strength\" + 0.022*\"lie\"\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "\n",
        "#Dataset\n",
        "texts = [\n",
        "    \"\"\"Siva is the most influential and respected leader in Guntur, shaping the community with his powerful vision.\n",
        "Known as the Iron Man of India, his presence commands respect and admiration across regions.\n",
        "Currently residing in the USA, Siva continues to inspire people globally, bridging the cultures of India and America.\n",
        "His journey from Guntur to the world stage is a testament to his dedication and resilience.\n",
        "Siva often says, \"True strength lies in uplifting others,\" a mantra he follows passionately.\n",
        "In both India and abroad, he remains a symbol of strength, unity, and unyielding determination.\"\"\"\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "processed_texts = []\n",
        "for doc in texts:\n",
        "    tokens = [token.lemma_ for token in nlp(doc.lower()) if token.is_alpha and not token.is_stop]\n",
        "    processed_texts.append(tokens)\n",
        "\n",
        "# Create Dictionary and Corpus\n",
        "dictionary = corpora.Dictionary(processed_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "# Determine Optimal Number of Topics\n",
        "coherence_scores = []\n",
        "for k in range(2, 6):\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=k, id2word=dictionary, random_state=42)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_scores.append((k, coherence_model.get_coherence()))\n",
        "\n",
        "# Select the model with the highest coherence score\n",
        "optimal_k = max(coherence_scores, key=lambda x: x[1])[0]\n",
        "\n",
        "# Train Final LDA Model\n",
        "final_lda_model = gensim.models.LdaModel(corpus, num_topics=optimal_k, id2word=dictionary, random_state=42)\n",
        "\n",
        "# Summarize Topics\n",
        "topics = final_lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(f\"Topic {topic[0]}: {topic[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "outputId": "dd42f71d-043f-486c-ae4c-66be43ce70c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.359*\"india\" + 0.359*\"siva\" + 0.239*\"strength\" + 0.239*\"guntur\" + 0.120*\"america\"\n"
          ]
        }
      ],
      "source": [
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "processed_texts = []\n",
        "for doc in texts:\n",
        "    tokens = [token.lemma_ for token in nlp(doc.lower()) if token.is_alpha and not token.is_stop]\n",
        "    processed_texts.append(tokens)\n",
        "\n",
        "# Create Dictionary and Corpus\n",
        "dictionary = corpora.Dictionary(processed_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "# Determine Optimal Number of Topics\n",
        "coherence_scores = []\n",
        "for k in range(2, 6):\n",
        "    lsi_model = models.LsiModel(corpus, num_topics=k, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lsi_model, texts=processed_texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_scores.append((k, coherence_model.get_coherence()))\n",
        "\n",
        "# Select the model with the highest coherence score\n",
        "optimal_k = max(coherence_scores, key=lambda x: x[1])[0]\n",
        "\n",
        "# Train Final LSA Model\n",
        "final_lsi_model = models.LsiModel(corpus, num_topics=optimal_k, id2word=dictionary)\n",
        "\n",
        "# Summarize Topics\n",
        "topics = final_lsi_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(f\"Topic {topic[0]}: {topic[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade gensim nltk\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('punkt')         # For tokenization\n",
        "nltk.download('stopwords')     # For stop words\n",
        "nltk.download('punkt_tab')     # To resolve punkt_tab issue\n",
        "\n",
        "# Step 3: Load and preprocess your dataset\n",
        "def preprocess(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())  # Tokenize the text\n",
        "    return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "# Example dataset\n",
        "documents = [\n",
        "    \"Natural language processing allows computers to understand human language.\",\n",
        "    \"Deep learning is a subset of machine learning.\",\n",
        "    \"Artificial intelligence is changing the world.\",\n",
        "    # Add more documents as needed...\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))  # Set of English stop words\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "\n",
        "from gensim import corpora\n",
        "\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import numpy as np\n",
        "\n",
        "num_topics_range = range(2, 20)\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in num_topics_range:\n",
        "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "\n",
        "best_num_topics = num_topics_range[np.argmax(coherence_scores)]\n",
        "print(f'Best number of topics: {best_num_topics}')\n",
        "\n",
        "final_model = LdaModel(corpus, num_topics=best_num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "\n",
        "topics = final_model.print_topics(num_words=10)\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f'Topic {i+1}: {topic[1]}')\n",
        "\n"
      ],
      "metadata": {
        "id": "VlAvuFJ9_cXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "outputId": "8d711ea1-e4f0-4fdb-9518-2809a9fa4c21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.39)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.6)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'BERTopic' object has no attribute 'get_document_embeddings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8e744f736faa>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Get document embeddings using the `get_document_embeddings` method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Ensure embeddings are 2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BERTopic' object has no attribute 'get_document_embeddings'"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required libraries (if not already installed)\n",
        "!pip install --upgrade bertopic\n",
        "\n",
        "# Step 2: Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from bertopic import BERTopic\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 3: Load a smaller subset of the dataset\n",
        "docs = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))['data'][:500]\n",
        "\n",
        "# Step 4: Fit BERTopic model with a smaller embedding model and reduced range of topics\n",
        "coherence_scores = []\n",
        "num_topics_range = range(2, 4)  # Testing only 2 and 3 topics\n",
        "\n",
        "for num_topics in num_topics_range:\n",
        "    topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", language=\"english\", nr_topics=num_topics)\n",
        "\n",
        "    topics, probs = topic_model.fit_transform(docs)\n",
        "\n",
        "    if len(set(topics)) > 1:\n",
        "        # Get document embeddings using the `get_document_embeddings` method\n",
        "        embeddings = topic_model.get_document_embeddings(docs)\n",
        "\n",
        "        # Ensure embeddings are 2D\n",
        "        if len(embeddings.shape) == 1:\n",
        "            embeddings = embeddings.reshape(-1, 1)\n",
        "\n",
        "        # Calculate silhouette score\n",
        "        silhouette_avg = silhouette_score(embeddings, topics)\n",
        "        coherence_scores.append(silhouette_avg)\n",
        "    else:\n",
        "        coherence_scores.append(-1)\n",
        "\n",
        "# Step 5: Find the optimal number of topics\n",
        "best_num_topics = num_topics_range[np.argmax(coherence_scores)]\n",
        "print(f'Best number of topics based on silhouette score: {best_num_topics}')\n",
        "\n",
        "# Fit final model with the best number of topics\n",
        "final_topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", language=\"english\", nr_topics=best_num_topics)\n",
        "final_topics, final_probs = final_topic_model.fit_transform(docs)\n",
        "\n",
        "# Step 6: Summarize the topics\n",
        "topic_info = final_topic_model.get_topic_info()\n",
        "print(topic_info.head(10))\n",
        "\n",
        "# Access and print the representative words for the first few topics\n",
        "for i in range(best_num_topics):\n",
        "    print(f'Topic {i}: {final_topic_model.get_topic(i)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3 (Alternative) - (10 points)**\n",
        "\n",
        "If you are unable to do the topic modeling using lda2vec, do the alternate question.\n",
        "\n",
        "Provide atleast 3 visualization for the topics generated by the BERTopic or LDA model. Explain each of the visualization in detail."
      ],
      "metadata": {
        "id": "Wslk2SYHML8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "# Then Explain the visualization\n",
        "\n",
        "# Repeat for the other 2 visualizations as well."
      ],
      "metadata": {
        "id": "eKZHcPjpNEDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating the four topic modeling algorithms—Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic—several factors come into play, including coherence scores, interpretability, and computational efficiency. LDA typically excels in small datasets, providing coherent and interpretable topics that clearly reflect underlying themes. In contrast, LSA may struggle with clarity and specificity, producing broader topics that can be less meaningful. lda2vec enhances topic representation by integrating word embeddings, yielding nuanced results, although its effectiveness can vary based on the quality of those embeddings. BERTopic stands out for larger, complex datasets, leveraging modern embeddings and clustering techniques to generate high-coherence and interpretable topics.\n",
        "\n",
        "Ultimately, the choice of algorithm depends on the context. For smaller datasets, LDA is often the preferred option due to its clarity and thematic focus. However, for larger datasets that demand richer semantic structures, BERTopic or lda2vec may be more effective. It's crucial to conduct thorough evaluations across different models and datasets, considering both coherence and interpretability to identify the most suitable approach for specific analytical goals."
      ],
      "metadata": {
        "id": "to3XZGvhNWWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating the four topic modeling algorithms—Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic—several factors come into play, including coherence scores, interpretability, and computational efficiency. LDA typically excels in small datasets, providing coherent and interpretable topics that clearly reflect underlying themes. In contrast, LSA may struggle with clarity and specificity, producing broader topics that can be less meaningful. lda2vec enhances topic representation by integrating word embeddings, yielding nuanced results, although its effectiveness can vary based on the quality of those embeddings. BERTopic stands out for larger, complex datasets, leveraging modern embeddings and clustering techniques to generate high-coherence and interpretable topics.\n",
        "\n",
        "Ultimately, the choice of algorithm depends on the context. For smaller datasets, LDA is often the preferred option due to its clarity and thematic focus. However, for larger datasets that demand richer semantic structures, BERTopic or lda2vec may be more effective. It's crucial to conduct thorough evaluations across different models and datasets, considering both coherence and interpretability to identify the most suitable approach for specific analytical goals."
      ],
      "metadata": {
        "id": "akVNxAl2HMQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was quite useful to work with text data and investigate topic modeling methods like as LDA, LSA, lda2vec, and BERTopic. My comprehension of feature extraction in text processing has improved as a result of each algorithm's own method for identifying themes and patterns in unstructured data. I was better able to understand the fundamental ideas of these algorithms and the variations in how they perceive themes and latent structures in the data thanks to the practical implementation. Insight into model tuning and the necessary balance between subject coherence and model complexity were made clear by the iterative process of selecting the best K values based on coherence scores.\n",
        "\n",
        "\n",
        "Since the lda2vec environment required certain installs and configurations, getting it up was one of the biggest obstacles. In many situations, lda2vec was less useful than alternative techniques because to its greater than anticipated training time and memory needs. BERTopic was easy to use, but it took more testing to adjust its settings to meet the coherence of other models. Choosing the best parameters, accurately preparing the text input, and meaningfully interpreting the results were all difficult tasks that required significant thought and patience."
      ],
      "metadata": {
        "id": "p2NbatstHG2A"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}