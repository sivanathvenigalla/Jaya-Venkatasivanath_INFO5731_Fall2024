{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivanathvenigalla/Jaya-Venkatasivanath_INFO5731_Fall2024/blob/main/Venigalla_jayavenkatasivanath_exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ9_vAVmYZp9"
      },
      "source": [
        "# **Required Intials**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM0FdP8XOKuk"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "current_percentage = 0\n",
        "def loading_animation(new_percentage, interval=0.05):\n",
        "    global current_percentage\n",
        "    if new_percentage < current_percentage:\n",
        "        raise ValueError(\"New percentage must be greater than or equal to the current percentage.\")\n",
        "    spinner = ['|', '/', '-', '\\\\']\n",
        "    num_frames = len(spinner)\n",
        "    while current_percentage < new_percentage:\n",
        "        frame = spinner[int((time.time() * 10) % num_frames)]\n",
        "        output_str = f'{frame} Loading... {current_percentage:.2f}%'\n",
        "        clear_output(wait=True)\n",
        "        display(output_str)\n",
        "        current_percentage += 1\n",
        "        time.sleep(interval)\n",
        "    if current_percentage >= 100:\n",
        "        clear_output(wait=True)\n",
        "        display('Done! 100%')\n",
        "    else:\n",
        "        clear_output(wait=True)\n",
        "        display(f'Loading... {current_percentage:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR9LyxFaWF4f"
      },
      "source": [
        "**Research Question** - What are the common information needs of users seeking academic resources related to Python programming?\n",
        "\n",
        "Data which needs to be collected -\n",
        "\n",
        "*   User information related - Extract common search queries related to       Python programming from search engines and Identify and categorize the types of academic resources users are seeking like tutorials, articles, books.\n",
        "*   Resource related - Gathering data on the content and descriptions of educational resources from academic websites and forums.Collecting common questions and topics from educational forums and community discussions.\n",
        "\n",
        "Amount of Data Needed for analysis -\n",
        "\n",
        "\n",
        "*   Collect data from at least 100-200 search queries to identify prevalent trends and topics.\n",
        "*   Gather data from at least 50-100 academic resources and 50-100 forum discussions to ensure a comprehensive analysis.\n",
        "*   Aim to collect a manageable dataset during the lecture period that includes key examples from each category to facilitate analysis.\n",
        "\n",
        "Steps for Collecting and Saving Data -\n",
        "\n",
        "\n",
        "1. Install Necessary Libraries: Ensure you have Python and the required libraries installed. You can install the necessary libraries using pip.\n",
        "2.   Data Collection :\n",
        "3. Write and Run Web Scraping Scripts\n",
        "4. Save and Manage Data\n",
        "5. Verify and Clean Data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cikVKDXdTbzE",
        "outputId": "9ee1beb7-bfe1-4320-9ddf-818f557a099e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to 'python_resources.csv'\n"
          ]
        }
      ],
      "source": [
        "# write your answer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the URL for web scraping\n",
        "url = 'https://www.example.com/python-programming-resources'\n",
        "\n",
        "# Send a request to the website\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Extract resource details\n",
        "resources = []\n",
        "for item in soup.find_all('div', class_='resource'):\n",
        "    title = item.find('h2').text\n",
        "    link = item.find('a')['href']\n",
        "    description = item.find('p').text\n",
        "    resources.append({'Title': title, 'Link': link, 'Description': description})\n",
        "\n",
        "# Convert to DataFrame and save to CSV\n",
        "df = pd.DataFrame(resources)\n",
        "df.to_csv('python_resources.csv', index=False)\n",
        "\n",
        "print(\"Data saved to 'python_resources.csv'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XvRknixTh1g",
        "outputId": "1173ebf7-0a0c-442a-ad46-8c9d8cb783ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error fetching https://www.example-academic-website.com/python-programming?page=1: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1f7f0>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=2: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=2 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3eaac80>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=3: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=3 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6ee30>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=4: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=4 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6d450>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=5: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=5 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6c550>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=6: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=6 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3eab490>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=7: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=7 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1f640>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=8: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=8 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1d660>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=9: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=9 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6cc40>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-academic-website.com/python-programming?page=10: HTTPSConnectionPool(host='www.example-academic-website.com', port=443): Max retries exceeded with url: /python-programming?page=10 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6ca60>: Failed to resolve 'www.example-academic-website.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=1: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6fc10>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=2: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=2 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3eab760>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=3: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=3 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1d9f0>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=4: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=4 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1c460>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=5: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=5 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1db70>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=6: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=6 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6e920>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=7: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=7 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f6d900>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=8: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=8 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1d5d0>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=9: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=9 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1e320>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Error fetching https://www.example-forum.com/python-questions?page=10: HTTPSConnectionPool(host='www.example-forum.com', port=443): Max retries exceeded with url: /python-questions?page=10 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7984e3f1ebc0>: Failed to resolve 'www.example-forum.com' ([Errno -2] Name or service not known)\"))\n",
            "Data saved to 'python_resources_and_discussions.csv'\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scrape_academic_resources(base_url, pages):\n",
        "    resources = []\n",
        "    for page in range(1, pages + 1):\n",
        "        url = f\"{base_url}?page={page}\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Check for HTTP errors\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            for item in soup.find_all('div', class_='resource-item'):\n",
        "                title = item.find('h2').text.strip()\n",
        "                link = item.find('a')['href'].strip()\n",
        "                description = item.find('p').text.strip()\n",
        "                resources.append({'Title': title, 'Link': link, 'Description': description})\n",
        "\n",
        "            time.sleep(1)  # Respectful scraping: wait a second between requests\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "\n",
        "    return resources\n",
        "\n",
        "def scrape_forum_discussions(base_url, pages):\n",
        "    discussions = []\n",
        "    for page in range(1, pages + 1):\n",
        "        url = f\"{base_url}?page={page}\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Check for HTTP errors\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            for post in soup.find_all('div', class_='post'):\n",
        "                title = post.find('h3').text.strip()\n",
        "                content = post.find('div', class_='content').text.strip()\n",
        "                discussions.append({'Title': title, 'Content': content})\n",
        "\n",
        "            time.sleep(1)  # Respectful scraping: wait a second between requests\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "\n",
        "    return discussions\n",
        "\n",
        "# Base URLs for scraping (Replace with real URLs)\n",
        "academic_base_url = 'https://www.example-academic-website.com/python-programming'\n",
        "forum_base_url = 'https://www.example-forum.com/python-questions'\n",
        "\n",
        "# Scrape academic resources (assuming 10 pages)\n",
        "academic_resources = scrape_academic_resources(academic_base_url, 10)\n",
        "\n",
        "# Scrape forum discussions (assuming 10 pages)\n",
        "forum_discussions = scrape_forum_discussions(forum_base_url, 10)\n",
        "\n",
        "# Combine datasets\n",
        "combined_data = academic_resources + forum_discussions\n",
        "\n",
        "# Ensure we only keep 1000 samples if more are collected\n",
        "if len(combined_data) > 1000:\n",
        "    combined_data = combined_data[:1000]\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(combined_data)\n",
        "df.to_csv('python_resources_and_discussions.csv', index=False)\n",
        "\n",
        "print(\"Data saved to 'python_resources_and_discussions.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdftuj2A3j68",
        "outputId": "c7e1d83c-9be8-42c5-f445-2adbe3151c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=0&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=1&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=2&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=3&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=4&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=5&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=6&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=7&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=8&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=9&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=10&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=11&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=12&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=13&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=14&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=15&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=16&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=17&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=18&pageSize=50\n",
            "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=19&pageSize=50\n",
            "1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Title': 'IT Risk Analysis Based on Risk Management Using ISO 31000: Case study Registration Application at University XYZ',\n",
              "  'journal': \"ICIBE '23: Proceedings of the 2023 9th International Conference on Industrial and Business Engineering\",\n",
              "  'Authros': ['Fahdi Saidi Lubis',\n",
              "   'Villy Satria Praditha',\n",
              "   'Muharman Lubis',\n",
              "   'Muhammad Fakhrul Safitra',\n",
              "   'Yumna Zahran Ramadhan'],\n",
              "  'Abstract': 'XYZ University has an Alpha Application used during the registration process for student courses each semester. There must be a hazard in the installation of this information system that could impede the process or functionality of the system; therefore, ...',\n",
              "  'Year': '2023'},\n",
              " {'Title': 'Aligning IT Asset Management at XYZ University with Infocom Standard and COBIT 2019 BAI09 Domain: Assessment and Design',\n",
              "  'journal': \"ICIBE '23: Proceedings of the 2023 9th International Conference on Industrial and Business Engineering\",\n",
              "  'Authros': ['Marcha Salsabila Afiani',\n",
              "   'Muharman Lubis',\n",
              "   'Ari Fajar Santoso',\n",
              "   'Muhammad Fakhrul Safitra'],\n",
              "  'Abstract': 'Information technology plays a very important role in a company or institution. Good information technology governance is required so that information technology can add value to companies and institutions. Every company or agency has assets that will ...',\n",
              "  'Year': '2023'},\n",
              " {'Title': 'Implementation of The Fuzzy Inference System to Determine The Amount of Purchase of Supplement Drug Products Based on Inventory and Sales Data at XYZ Pharmacy',\n",
              "  'journal': \"ICONETSI '22: Proceedings of the 2022 International Conference on Engineering and Information Technology for Sustainable Industry\",\n",
              "  'Authros': ['Fani Puspitasari', 'Sofia Debi Puspa', 'Christian Kenny Verel'],\n",
              "  'Abstract': 'The Health Service Industry is one of the important and crucial industries to be maintained in order to support the welfare of the community. XYZ Pharmacy, which is located in the Jakarta area, is one of the industrial companies engaged in Health ...',\n",
              "  'Year': '2022'},\n",
              " {'Title': 'Data Governance to Improve Data Quality for Statistical Process Control (SPC): A Case Study PT. XYZ',\n",
              "  'journal': \"ICSIM '21: Proceedings of the 2021 4th International Conference on Software Engineering and Information Management\",\n",
              "  'Authros': ['Mulyani Pratiwi', 'Yova Ruldeviyani'],\n",
              "  'Abstract': 'PT XYZ is having difficulty to implement a continuous monitoring system to be able to quickly identify problems in the process and be able to fix them immediately. The monitoring system is carried out by the Statistical Process Control (SPC) especially ...',\n",
              "  'Year': '2021'},\n",
              " {'Title': 'Importance of Internalization of Tacit Knowledge Sharing in Project Management Case in XYZ Finance',\n",
              "  'journal': \"ICONETSI '20: Proceedings of the 2020 International Conference on Engineering and Information Technology for Sustainable Industry\",\n",
              "  'Authros': ['Ryant Herwansyah'],\n",
              "  'Abstract': 'Tacit Knowledge is an essential part of the project, especially in a software development project. Because project without tacit knowledge sharing can lead to project failure. And there is concern in XYZ Finance Project did not have a good tacit ...',\n",
              "  'Year': '2020'},\n",
              " {'Title': 'The xyz algorithm for fast interaction search in high-dimensional data',\n",
              "  'journal': 'The Journal of Machine Learning Research (JMLR), Volume 19, Issue 1',\n",
              "  'Authros': ['Gian-Andrea Thanei', 'Nicolai Meinshausen', 'Rajen D. Shah'],\n",
              "  'Abstract': 'When performing regression on a data set with p variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed ...',\n",
              "  'Year': '2018'},\n",
              " {'Title': 'Demand Forecasting for Drinking Water Products to Reduce Gap Between Estimation and Realization of Demand Using Artificial Neural Network (ANN) Methods in PT. XYZ',\n",
              "  'journal': \"ICONETSI '20: Proceedings of the 2020 International Conference on Engineering and Information Technology for Sustainable Industry\",\n",
              "  'Authros': ['Rizka Cahya Syafitri', 'Ari Yanuar Ridwan', 'Nia Novitasari'],\n",
              "  'Abstract': 'Supply Chain has components such as vendors, manufacturers, factories, warehouses retailers, customers, etc. Every relationship between components must have good information in order to create informed business decisions. Sales forecast are part of a ...',\n",
              "  'Year': '2020'},\n",
              " {'Title': 'XYZ-Randomization using TSVs for Low-Latency Energy Efficient 3D-NoCs',\n",
              "  'journal': \"NOCS '17: Proceedings of the Eleventh IEEE/ACM International Symposium on Networks-on-Chip\",\n",
              "  'Authros': ['H. Nakahara', 'Ng.Anh Vu Doan', 'R. Yasudo', 'H. Amano'],\n",
              "  'Abstract': 'In this paper, we propose a method to design low latency and low energy networks for 3D Network-on-Chip (3D-NoC). Recent many-core processors require low-latency interconnection networks since the increasing number of cores limits the network ...',\n",
              "  'Year': '2017'},\n",
              " {'Title': 'The CMMI-Dev Implementation Factors for Software Quality Improvement: A Case of XYZ Corporation',\n",
              "  'journal': \"APIT '20: Proceedings of the 2020 2nd Asia Pacific Information Technology Conference\",\n",
              "  'Authros': ['Yuki Alqadri',\n",
              "   'Eko K. Budiardjo',\n",
              "   'Alex Ferdinansyah',\n",
              "   'Mokhammad F. Rokhman'],\n",
              "  'Abstract': 'Refer to CMMI-Institute Appraisal result, there are 6 (six) companies in Indonesia that have achieved CMMI-Dev v1.3 maturity level 3, one of them is XYZ Corporation. Initially, XYZ Corporation has set a target to achieve CMMI-Dev level 2. As advised by ...',\n",
              "  'Year': '2020'},\n",
              " {'Title': 'Evaluating Ad Creative and Web Context Alignment with Attention Measurement',\n",
              "  'journal': \"ICCDA '23: Proceedings of the 2023 7th International Conference on Computing and Data Analysis\",\n",
              "  'Authros': ['John Hawkins', 'Graham Burton'],\n",
              "  'Abstract': 'Contextual targeting is a common strategy that places marketing messages in media locations that are aligned with a target audience. The challenge of contextual targeting is knowing the ideal schema and the set of categories that provide the right ...',\n",
              "  'Year': '2023'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "keyword = \"XYZ\"\n",
        "start_year = \"2014\"\n",
        "end_year = \"2024\"\n",
        "max_articles = 1000\n",
        "articles =[]\n",
        "\n",
        "for i in range(20):\n",
        "  base_url = \"https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=dl&AfterMonth=1&AfterYear=2014&BeforeMonth=12&BeforeYear=2024&AllField=xyz&startPage=\"+str(i)+\"&pageSize=50\"\n",
        "  print(base_url)\n",
        "  response = requests.get(base_url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  authors = []\n",
        "  c = 0\n",
        "  for item in soup.find_all('li', class_='search__item issue-item-container'):\n",
        "    title = item.find('h5', class_='issue-item__title').text.strip()\n",
        "    #print(title)\n",
        "    date = item.find('div', class_=\"bookPubDate simple-tooltip__block--b\").text.strip()\n",
        "    #print(date.split()[-1])\n",
        "    try:\n",
        "      journal = item.find('span', class_ = \"epub-section__title\").text.strip()\n",
        "    except:\n",
        "      journal = \"Not Available on website\"\n",
        "    #print(journal)\n",
        "    try:\n",
        "      abstract = item.find('div', class_ = \"issue-item__abstract truncate-text\").text.strip()\n",
        "    except:\n",
        "      abstract = \"Not Available on website\"\n",
        "    #print(abstract)\n",
        "    try:\n",
        "      authors = []\n",
        "      for j in item.find_all(\"span\",class_ = 'hlFld-ContribAuthor'):\n",
        "        authors.append(j.find('a').text.strip())\n",
        "    except:\n",
        "      authors = \"Not Available on website\"\n",
        "    #print(authors)\n",
        "    articles.append({'Title': title, \"journal\": journal, \"Authros\": authors, \"Abstract\": abstract, \"Year\": date.split()[-1] })\n",
        "  time.sleep((30-5)*np.random.random()+5) #from 5 to 30 seconds #https://stackoverflow.com/questions/45193277/scraping-large-amount-of-google-scholar-pages-with-url\n",
        "with open(\"articles.json\", \"w\") as fp:\n",
        "    json.dump(articles, fp)\n",
        "print(len(articles))\n",
        "articles[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctHjouz2P4iX",
        "outputId": "2ab44173-3953-4d2e-9283-699f8767fd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Status code 403\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have gained a deeper grasp of HTML structure, the DOM, and web requests by working on web scraping chores. This has been an invaluable learning experience. My ability to collect data was enhanced by learning to handle JSON from APIs, and I found that programs like Beautiful Soup and Scrapy were especially useful for quickly extracting data. Utilizing browser automation technologies like Selenium, I was able to overcome challenges posed by websites that used anti-scraping techniques. Import.io and similar technologies would have offered a simple and quick way to extract data if I had gone with a non-coding method. All things considered, the capacity to collect and evaluate internet data is extremely pertinent to my area of work, providing me with up-to-date information on consumer trends and behavior that supports my research and, in the end, improves my analytical abilities for new and ongoing projects.\n"
      ],
      "metadata": {
        "id": "ZMi3CWPzbNry"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "sZOhks1dXWEe"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}